{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "import collections\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition import qExpectedImprovement\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.generation import MaxPosteriorSampling\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.test_functions import Ackley\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.priors import HorseshoePrior\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "10 512 4000\n"
     ]
    }
   ],
   "source": [
    "fun = Ackley(dim=20, negate=True).to(dtype=dtype, device=device)\n",
    "fun.bounds[0, :].fill_(-5)\n",
    "fun.bounds[1, :].fill_(10)\n",
    "dim = fun.dim\n",
    "lb, ub = fun.bounds\n",
    "print(dim)\n",
    "\n",
    "batch_size = 4\n",
    "n_init = 2 * dim\n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky\n",
    "\n",
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
    "N_CANDIDATES = min(5000, max(2000, 200 * dim)) if not SMOKE_TEST else 4\n",
    "\n",
    "print(NUM_RESTARTS, RAW_SAMPLES, N_CANDIDATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_objective(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return fun(unnormalize(x, fun.bounds))\n",
    "\n",
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP-EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qExpectedImprovement(\n",
      "  (model): SingleTaskGP(\n",
      "    (likelihood): GaussianLikelihood(\n",
      "      (noise_covar): HomoskedasticNoise(\n",
      "        (raw_noise_constraint): Interval(1.000E-08, 1.000E-03)\n",
      "      )\n",
      "    )\n",
      "    (mean_module): ConstantMean()\n",
      "    (covar_module): ScaleKernel(\n",
      "      (base_kernel): MaternKernel(\n",
      "        (lengthscale_prior): GammaPrior()\n",
      "        (raw_lengthscale_constraint): Positive()\n",
      "      )\n",
      "      (outputscale_prior): GammaPrior()\n",
      "      (raw_outputscale_constraint): Positive()\n",
      "    )\n",
      "  )\n",
      "  (objective): IdentityMCObjective()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "X_ei = get_initial_points(dim, n_init)\n",
    "Y_ei = torch.tensor(\n",
    "    [eval_objective(x) for x in X_ei], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "while len(Y_ei) < 400:\n",
    "    train_Y = (Y_ei - Y_ei.mean()) / Y_ei.std()\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    model = SingleTaskGP(X_ei, train_Y, likelihood=likelihood)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_mll(mll)\n",
    "\n",
    "    # Create a batch\n",
    "    ei = qExpectedImprovement(model, train_Y.max())\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        ei,\n",
    "        bounds=torch.stack(\n",
    "            [\n",
    "                torch.zeros(dim, dtype=dtype, device=device),\n",
    "                torch.ones(dim, dtype=dtype, device=device),\n",
    "            ]\n",
    "        ),\n",
    "        q=batch_size,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "    )\n",
    "\n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    # Append data\n",
    "    X_ei = torch.cat((X_ei, candidate), axis=0)\n",
    "    Y_ei = torch.cat((Y_ei, Y_next), axis=0)\n",
    "    # print(X_ei.size(), Y_ei.size())\n",
    "\n",
    "    # Print current status\n",
    "    print(f\"{len(X_ei)}) Best value: {Y_ei.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Optional\n",
    "\n",
    "# from botorch import acquisition\n",
    "# from botorch.posteriors import Posterior\n",
    "# from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "# from botorch.models import SingleTaskGP\n",
    "# from botorch.models.model import Model\n",
    "# from botorch.models.transforms import Warp\n",
    "# from botorch.models.transforms.outcome import OutcomeTransform\n",
    "# from botorch.optim import optimize_acqf\n",
    "# from botorch.optim.utils import _filter_kwargs\n",
    "# from botorch.sampling import SobolQMCNormalSampler\n",
    "\n",
    "# import gpytorch\n",
    "# from gpytorch.models import ApproximateGP\n",
    "# from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "# from torch import Tensor\n",
    "\n",
    "\n",
    "# class SVGP(ApproximateGP):\n",
    "#     def __init__(self, inducing_points: Tensor, covar_module: Optional[gpytorch.kernels.Kernel] = None):\n",
    "#         variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "#         variational_strategy = VariationalStrategy(self, \n",
    "#                                                    inducing_points, \n",
    "#                                                    variational_distribution,\n",
    "#                                                    learn_inducing_locations=True)\n",
    "#         super(SVGP, self).__init__(variational_strategy)\n",
    "#         self.mean_module = gpytorch.means.ConstantMean()\n",
    "#         if covar_module is None:\n",
    "#             self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "#         else:\n",
    "#             self.covar_module = covar_module\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "#     def posterior(self, x: Tensor):\n",
    "#         self.eval()\n",
    "#         return self.forward(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpytorch\n",
    "# from botorch.models.approximate_gp import ApproximateGPyTorchModel\n",
    "# from gpytorch.models import ApproximateGP\n",
    "# from gpytorch.variational import CholeskyVariationalDistribution\n",
    "# from gpytorch.variational import VariationalStrategy\n",
    "# from botorch.posteriors.gpytorch import GPyTorchPosterior\n",
    "# import torch\n",
    "\n",
    "# class GPModel(ApproximateGP):\n",
    "#     def __init__(self, inducing_points, likelihood):\n",
    "#         variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0) )\n",
    "#         variational_strategy = VariationalStrategy(\n",
    "#             self,\n",
    "#             inducing_points,\n",
    "#             variational_distribution,\n",
    "#             learn_inducing_locations=True\n",
    "#             )\n",
    "#         super(GPModel, self).__init__(variational_strategy)\n",
    "#         self.mean_module = gpytorch.means.ConstantMean()\n",
    "#         self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "#         self.num_outputs = 1\n",
    "#         self.likelihood = likelihood \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "#     def posterior(\n",
    "#             self, X, output_indices=None, observation_noise=False, *args, **kwargs\n",
    "#         ) -> GPyTorchPosterior:\n",
    "#             self.eval()  # make sure model is in eval mode\n",
    "#             # self.model.eval()\n",
    "#             self.likelihood.eval()\n",
    "#             dist = self.likelihood(self(X)) \n",
    "\n",
    "#             return GPyTorchPosterior(mvn=dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpytorch.mlls import PredictiveLogLikelihood \n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# X_sparse = get_initial_points(dim, n_init)\n",
    "# Y_sparse = torch.tensor(\n",
    "#     [eval_objective(x) for x in X_sparse], dtype=dtype, device=device\n",
    "# ).unsqueeze(-1)\n",
    "\n",
    "# likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "# model = GPModel(X_sparse, likelihood=likelihood)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# while len(Y_sparse) < 400:\n",
    "#     train_Y = (Y_sparse - Y_sparse.mean()) / Y_sparse.std()\n",
    "\n",
    "#     model = GPModel(X_sparse, likelihood=likelihood)\n",
    "#     mll = PredictiveLogLikelihood(model.likelihood, model, num_data=X_sparse.shape[0])\n",
    "    \n",
    "#     pred = model(X_sparse)\n",
    "#     loss = -mll(pred, train_Y)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.mean().backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Create a batch\n",
    "#     ei = qExpectedImprovement(model, train_Y.max())\n",
    "#     candidate, acq_value = optimize_acqf(\n",
    "#         ei,\n",
    "#         bounds=torch.stack(\n",
    "#             [\n",
    "#                 torch.zeros(dim, dtype=dtype, device=device),\n",
    "#                 torch.ones(dim, dtype=dtype, device=device),\n",
    "#             ]\n",
    "#         ),\n",
    "#         q=batch_size,\n",
    "#         num_restarts=NUM_RESTARTS,\n",
    "#         raw_samples=RAW_SAMPLES,\n",
    "#     )\n",
    "\n",
    "#     Y_next = torch.tensor(\n",
    "#         [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "#     ).unsqueeze(-1)\n",
    "\n",
    "#     # Append data\n",
    "#     X_sparse = torch.cat((X_sparse, candidate), axis=0)\n",
    "#     Y_sparse = torch.cat((Y_sparse, Y_next), axis=0)\n",
    "#     # print(X_ei.size(), Y_ei.size())\n",
    "\n",
    "#     # Print current status\n",
    "#     print(f\"{len(X_sparse)}) Best value: {Y_sparse.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44) Best value: -5.14e+00\n",
      "48) Best value: -5.14e+00\n",
      "52) Best value: -5.14e+00\n",
      "56) Best value: -5.14e+00\n",
      "60) Best value: -5.14e+00\n",
      "64) Best value: -5.14e+00\n",
      "68) Best value: -5.14e+00\n",
      "72) Best value: -5.14e+00\n",
      "76) Best value: -5.14e+00\n",
      "80) Best value: -5.14e+00\n",
      "84) Best value: -5.14e+00\n",
      "88) Best value: -5.14e+00\n",
      "92) Best value: -5.14e+00\n",
      "96) Best value: -5.14e+00\n",
      "100) Best value: -5.14e+00\n",
      "104) Best value: -5.14e+00\n",
      "108) Best value: -5.14e+00\n",
      "112) Best value: -5.14e+00\n",
      "116) Best value: -5.14e+00\n",
      "120) Best value: -5.14e+00\n",
      "124) Best value: -5.14e+00\n",
      "128) Best value: -5.14e+00\n",
      "132) Best value: -5.14e+00\n",
      "136) Best value: -5.14e+00\n",
      "140) Best value: -5.14e+00\n",
      "144) Best value: -5.14e+00\n",
      "148) Best value: -5.14e+00\n",
      "152) Best value: -5.14e+00\n",
      "156) Best value: -5.14e+00\n",
      "160) Best value: -5.14e+00\n",
      "164) Best value: -5.14e+00\n",
      "168) Best value: -5.14e+00\n",
      "172) Best value: -5.14e+00\n",
      "176) Best value: -5.14e+00\n"
     ]
    },
    {
     "ename": "ModelFittingError",
     "evalue": "All attempts to fit the model have failed. For more information, try enabling botorch.settings.debug mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModelFittingError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m SingleTaskGP(X_sparse, train_Y, covar_module\u001b[38;5;241m=\u001b[39mcovar_module, likelihood\u001b[38;5;241m=\u001b[39mlikelihood)\n\u001b[0;32m     29\u001b[0m mll \u001b[38;5;241m=\u001b[39m ExactMarginalLogLikelihood(model\u001b[38;5;241m.\u001b[39mlikelihood, model)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mfit_gpytorch_mll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Create a batch\u001b[39;00m\n\u001b[0;32m     33\u001b[0m ei \u001b[38;5;241m=\u001b[39m qExpectedImprovement(model, train_Y\u001b[38;5;241m.\u001b[39mmax())\n",
      "File \u001b[1;32mc:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\botorch\\fit.py:105\u001b[0m, in \u001b[0;36mfit_gpytorch_mll\u001b[1;34m(mll, closure, optimizer, closure_kwargs, optimizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# defer to per-method defaults\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FitGPyTorchMLL(\n\u001b[0;32m    106\u001b[0m     mll,\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mtype\u001b[39m(mll\u001b[38;5;241m.\u001b[39mlikelihood),\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mtype\u001b[39m(mll\u001b[38;5;241m.\u001b[39mmodel),\n\u001b[0;32m    109\u001b[0m     closure\u001b[38;5;241m=\u001b[39mclosure,\n\u001b[0;32m    110\u001b[0m     closure_kwargs\u001b[38;5;241m=\u001b[39mclosure_kwargs,\n\u001b[0;32m    111\u001b[0m     optimizer_kwargs\u001b[38;5;241m=\u001b[39moptimizer_kwargs,\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    113\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\botorch\\utils\\dispatcher.py:93\u001b[0m, in \u001b[0;36mDispatcher.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(types\u001b[38;5;241m=\u001b[39mtypes)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MDNotImplementedError:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Traverses registered methods in order, yields whenever a match is found\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     funcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_iter(\u001b[38;5;241m*\u001b[39mtypes)\n",
      "File \u001b[1;32mc:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\botorch\\fit.py:283\u001b[0m, in \u001b[0;36m_fit_fallback\u001b[1;34m(mll, _, __, closure, optimizer, closure_kwargs, optimizer_kwargs, max_attempts, warning_handler, caught_exception_types, **ignore)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug\u001b[38;5;241m.\u001b[39moff():\n\u001b[0;32m    281\u001b[0m     msg \u001b[38;5;241m=\u001b[39m msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m For more information, try enabling botorch.settings.debug mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ModelFittingError(msg)\n",
      "\u001b[1;31mModelFittingError\u001b[0m: All attempts to fit the model have failed. For more information, try enabling botorch.settings.debug mode."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel, MaternKernel, Kernel\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X_sparse = get_initial_points(dim, n_init)\n",
    "Y_sparse = torch.tensor(\n",
    "    [eval_objective(x) for x in X_sparse], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "\n",
    "n_inducing_points = 5\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=n_inducing_points,\n",
    "    batch_size=min(10000, X_sparse.shape[0]),\n",
    ")\n",
    "kmeans.fit(X_sparse.cpu().numpy())\n",
    "inducing_points = torch.from_numpy(kmeans.cluster_centers_.copy())\n",
    "\n",
    "likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "base_covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "covar_module = InducingPointKernel(base_covar_module, inducing_points=inducing_points, likelihood=likelihood)\n",
    "\n",
    "while len(Y_sparse) < 400:\n",
    "    train_Y = (Y_sparse - Y_sparse.mean()) / Y_sparse.std()\n",
    "    \n",
    "    model = SingleTaskGP(X_sparse, train_Y, covar_module=covar_module, likelihood=likelihood)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_mll(mll)\n",
    "    \n",
    "    # Create a batch\n",
    "    ei = qExpectedImprovement(model, train_Y.max())\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        ei,\n",
    "        bounds=torch.stack(\n",
    "            [\n",
    "                torch.zeros(dim, dtype=dtype, device=device),\n",
    "                torch.ones(dim, dtype=dtype, device=device),\n",
    "            ]\n",
    "        ),\n",
    "        q=batch_size,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "    )\n",
    "\n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    # Append data\n",
    "    X_sparse = torch.cat((X_sparse, candidate), axis=0)\n",
    "    Y_sparse = torch.cat((Y_sparse, Y_next), axis=0)\n",
    "    # print(X_ei.size(), Y_ei.size())\n",
    "\n",
    "    # Print current status\n",
    "    print(f\"{len(X_sparse)}) Best value: {Y_sparse.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from botorch.models import SingleTaskVariationalGP\n",
    "# from gpytorch.mlls import VariationalELBO\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# X_sparse = get_initial_points(dim, n_init)\n",
    "# Y_sparse = torch.tensor(\n",
    "#     [eval_objective(x) for x in X_sparse], dtype=dtype, device=device\n",
    "# ).unsqueeze(-1)\n",
    "\n",
    "# likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "\n",
    "# while len(Y_sparse) < 400:\n",
    "#     train_Y = (Y_sparse - Y_sparse.mean()) / Y_sparse.std()\n",
    "\n",
    "#     model = SingleTaskVariationalGP(X_sparse, train_Y, likelihood=likelihood)\n",
    "#     # mll = VariationalELBO(model.likelihood, model.model, num_data=X_sparse.shape[0])\n",
    "#     mll = ExactMarginalLogLikelihood(model.likelihood, model.model)\n",
    "#     fit_gpytorch_mll(mll)\n",
    "    \n",
    "#     # Create a batch\n",
    "#     ei = qExpectedImprovement(model, train_Y.max())\n",
    "#     candidate, acq_value = optimize_acqf(\n",
    "#         ei,\n",
    "#         bounds=torch.stack(\n",
    "#             [\n",
    "#                 torch.zeros(dim, dtype=dtype, device=device),\n",
    "#                 torch.ones(dim, dtype=dtype, device=device),\n",
    "#             ]\n",
    "#         ),\n",
    "#         q=batch_size,\n",
    "#         num_restarts=NUM_RESTARTS,\n",
    "#         raw_samples=RAW_SAMPLES,\n",
    "#     )\n",
    "\n",
    "#     Y_next = torch.tensor(\n",
    "#         [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "#     ).unsqueeze(-1)\n",
    "\n",
    "#     # Append data\n",
    "#     X_sparse = torch.cat((X_sparse, candidate), axis=0)\n",
    "#     Y_sparse = torch.cat((Y_sparse, Y_next), axis=0)\n",
    "#     # print(X_ei.size(), Y_ei.size())\n",
    "\n",
    "#     # Print current status\n",
    "#     print(f\"{len(X_sparse)}) Best value: {Y_sparse.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NP-MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from math import pi\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class My_Ackley(Dataset):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "\n",
    "        # Generate data\n",
    "        self.data = []\n",
    "        self.data.append((self.train_x, self.train_y))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_np = get_initial_points(dim, n_init)\n",
    "# Y_np = torch.tensor(\n",
    "#     [eval_objective(x) for x in X_np], dtype=dtype, device=device\n",
    "# ).unsqueeze(-1)\n",
    "\n",
    "# ackley_dataset = My_Ackley(train_x = X_np, train_y = Y_np)\n",
    "# data_loader = DataLoader(ackley_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# for _, i in enumerate(data_loader):\n",
    "#     print(i[0].shape)\n",
    "#     print(i[1].shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200, loss 28.024\n",
      "48) Best value: -1.24e+01\n",
      "iteration 200, loss 22.455\n",
      "56) Best value: -1.17e+01\n",
      "iteration 200, loss 18.247\n",
      "64) Best value: -1.17e+01\n",
      "iteration 200, loss 15.737\n",
      "72) Best value: -1.17e+01\n",
      "iteration 200, loss 14.828\n",
      "80) Best value: -1.17e+01\n",
      "iteration 200, loss 24.107\n",
      "88) Best value: -1.17e+01\n",
      "iteration 200, loss 16.874\n",
      "96) Best value: -1.17e+01\n",
      "iteration 200, loss 19.728\n",
      "104) Best value: -1.17e+01\n",
      "iteration 200, loss 31.477\n",
      "112) Best value: -1.17e+01\n",
      "iteration 200, loss 23.219\n",
      "120) Best value: -1.17e+01\n",
      "iteration 200, loss 22.382\n",
      "128) Best value: -1.17e+01\n",
      "iteration 200, loss 13.183\n",
      "136) Best value: -1.17e+01\n",
      "iteration 200, loss 5.858\n",
      "144) Best value: -1.16e+01\n",
      "iteration 200, loss 21.742\n",
      "152) Best value: -1.16e+01\n",
      "iteration 200, loss 21.042\n",
      "160) Best value: -1.16e+01\n",
      "iteration 200, loss 13.620\n",
      "168) Best value: -1.16e+01\n",
      "iteration 200, loss -0.981\n",
      "176) Best value: -1.12e+01\n",
      "iteration 200, loss 3.853\n",
      "184) Best value: -1.12e+01\n",
      "iteration 200, loss 6.343\n",
      "192) Best value: -1.12e+01\n",
      "iteration 200, loss 14.153\n",
      "200) Best value: -1.12e+01\n",
      "iteration 200, loss 13.038\n",
      "208) Best value: -9.92e+00\n",
      "iteration 200, loss -8.626\n",
      "216) Best value: -9.92e+00\n",
      "iteration 200, loss 2.981\n",
      "224) Best value: -9.92e+00\n",
      "iteration 200, loss 4.926\n",
      "232) Best value: -9.92e+00\n",
      "iteration 200, loss -16.927\n",
      "240) Best value: -9.92e+00\n",
      "iteration 200, loss 9.076\n",
      "248) Best value: -9.92e+00\n",
      "iteration 200, loss 36.669\n",
      "256) Best value: -9.92e+00\n",
      "iteration 200, loss -13.429\n",
      "264) Best value: -9.92e+00\n",
      "iteration 200, loss -10.592\n",
      "272) Best value: -9.92e+00\n",
      "iteration 200, loss 10.858\n",
      "280) Best value: -9.92e+00\n",
      "iteration 200, loss -1.819\n",
      "288) Best value: -9.92e+00\n",
      "iteration 200, loss 0.057\n",
      "296) Best value: -9.92e+00\n",
      "iteration 200, loss 21.577\n",
      "304) Best value: -9.92e+00\n",
      "iteration 200, loss -11.458\n",
      "312) Best value: -9.92e+00\n",
      "iteration 200, loss 1.591\n",
      "320) Best value: -9.92e+00\n",
      "iteration 200, loss -15.656\n",
      "328) Best value: -9.92e+00\n",
      "iteration 200, loss -4.854\n",
      "336) Best value: -9.92e+00\n",
      "iteration 200, loss -35.494\n",
      "344) Best value: -9.92e+00\n",
      "iteration 200, loss 68.833\n",
      "352) Best value: -9.92e+00\n",
      "iteration 200, loss -10.345\n",
      "360) Best value: -9.92e+00\n",
      "iteration 200, loss -20.601\n",
      "368) Best value: -9.92e+00\n",
      "iteration 200, loss -43.477\n",
      "376) Best value: -9.92e+00\n",
      "iteration 200, loss 6.956\n",
      "384) Best value: -9.92e+00\n",
      "iteration 200, loss -18.846\n",
      "392) Best value: -9.92e+00\n",
      "iteration 200, loss -37.529\n",
      "400) Best value: -9.92e+00\n"
     ]
    }
   ],
   "source": [
    "## bo\n",
    "from torch.utils.data import DataLoader\n",
    "from neural_process import NeuralProcess\n",
    "from training import NeuralProcessTrainer\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from utils import context_target_split\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.sampling.stochastic_samplers import StochasticSampler\n",
    "from botorch.acquisition.objective import IdentityMCObjective\n",
    "\n",
    "## NP model\n",
    "x_dim = 20\n",
    "y_dim = 1\n",
    "r_dim = 50  # Dimension of representation of context points\n",
    "z_dim = 50  # Dimension of sampled latent variable\n",
    "h_dim = 50  # Dimension of hidden layers in encoder and decoder\n",
    "neuralprocess = NeuralProcess(x_dim, y_dim, r_dim, z_dim, h_dim)\n",
    "optimizer = torch.optim.Adam(neuralprocess.parameters(), lr=3e-4)\n",
    "\n",
    "train_batch_size = 1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X_np = get_initial_points(dim, n_init)\n",
    "Y_np = torch.tensor(\n",
    "    [eval_objective(x) for x in X_np], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "sampler = StochasticSampler(torch.Size([batch_size*2]), seed=1234)\n",
    "obj = IdentityMCObjective()\n",
    "\n",
    "while len(Y_np) < 400:\n",
    "    best_f = Y_np.max() \n",
    "    train_Y = (Y_np - Y_np.mean()) / Y_np.std()\n",
    "    ackley_dataset = My_Ackley(train_x = X_np, train_y = train_Y)\n",
    "    data_loader = DataLoader(ackley_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    num_context = X_np.shape[0] // 4\n",
    "    num_target = X_np.shape[0] // 4\n",
    "    np_trainer = NeuralProcessTrainer(device, neuralprocess, optimizer,\n",
    "                                    num_context_range=(num_context, num_context),\n",
    "                                    num_extra_target_range=(num_target, num_target), \n",
    "                                    print_freq=200)\n",
    "\n",
    "    neuralprocess.training = True\n",
    "    np_trainer.train(data_loader, 200)\n",
    "\n",
    "    # Create a batch\n",
    "    neuralprocess.training = False\n",
    "    for batch in data_loader:\n",
    "        break\n",
    "    x, y = batch\n",
    "    x_context, y_context, _, _ = context_target_split(x[0:1], y[0:1], \n",
    "                                                    num_context, \n",
    "                                                    num_target)\n",
    "    neuralprocess.set_context_for_posterior(x_context, y_context)\n",
    "\n",
    "    ei = qExpectedImprovement(neuralprocess, best_f, sampler, obj)\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        ei,\n",
    "        bounds=torch.stack(\n",
    "            [\n",
    "                torch.zeros(dim, dtype=dtype, device=device),\n",
    "                torch.ones(dim, dtype=dtype, device=device),\n",
    "            ]\n",
    "        ),\n",
    "        q=batch_size*2, # The number of candidates\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES, # The number of samples for initialization.\n",
    "    )\n",
    "    # break\n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    # Append data\n",
    "    X_np = torch.cat((X_np, candidate), axis=0)\n",
    "    Y_np = torch.cat((Y_np, Y_next), axis=0)\n",
    "\n",
    "    # Print current status\n",
    "    print(f\"{len(X_np)}) Best value: {Y_np.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGP-qEI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparse-GP-qEI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNP-qEI-stochasticsampler-bc4\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# , \"EI\", \"Sobol\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m runs \u001b[38;5;241m=\u001b[39m [Y_ei, Y_sparse, \u001b[43mY_np\u001b[49m] \u001b[38;5;66;03m# , Y_ei, Y_Sobol\u001b[39;00m\n\u001b[0;32m     10\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(names, runs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "names = [\"GP-qEI\", \"Sparse-GP-qEI\", \"NP-qEI-stochasticsampler-bc4\"] # , \"EI\", \"Sobol\"\n",
    "runs = [Y_ei, Y_sparse, Y_np] # , Y_ei, Y_Sobol\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, run in zip(names, runs):\n",
    "    fx = np.maximum.accumulate(run.cpu())\n",
    "    plt.plot(fx, marker=\"\", lw=3)\n",
    "\n",
    "plt.plot([0, len(Y_ei)], [fun.optimal_value, fun.optimal_value], \"k--\", lw=3)\n",
    "plt.xlabel(\"Function value\", fontsize=18)\n",
    "plt.xlabel(\"Number of evaluations\", fontsize=18)\n",
    "plt.title(\"20D Ackley\", fontsize=24)\n",
    "plt.xlim([0, len(Y_ei)])\n",
    "plt.ylim([-15, 1])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend(\n",
    "    names + [\"Global optimal value\"],\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0, -0.08, 1, 1),\n",
    "    bbox_transform=plt.gcf().transFigure,\n",
    "    ncol=4,\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTIVARIATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from models import Encoder, MuSigmaEncoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from models import Encoder, MuSigmaEncoder, Decoder\n",
    "# from torch import nn\n",
    "# from torch.distributions import Normal\n",
    "# from utils import img_mask_to_np_input\n",
    "# from botorch.posteriors.deterministic import DeterministicPosterior\n",
    "\n",
    "# #\n",
    "# import itertools\n",
    "# import warnings\n",
    "# from abc import ABC\n",
    "# from copy import deepcopy\n",
    "# from typing import Any, List, Optional, Tuple, TYPE_CHECKING, Union\n",
    "\n",
    "# import torch\n",
    "# from botorch.acquisition.objective import PosteriorTransform\n",
    "# from botorch.models.utils import (\n",
    "#     _make_X_full,\n",
    "#     add_output_dim,\n",
    "#     gpt_posterior_settings,\n",
    "#     mod_batch_shape,\n",
    "#     multioutput_to_batch_mode_transform,\n",
    "# )\n",
    "\n",
    "# import gpytorch\n",
    "# from gpytorch.distributions import MultitaskMultivariateNormal, MultivariateNormal\n",
    "# from gpytorch.likelihoods.gaussian_likelihood import FixedNoiseGaussianLikelihood\n",
    "# from torch import Tensor\n",
    "\n",
    "# from botorch.posteriors.transformed import TransformedPosterior  # pragma: no cover\n",
    "\n",
    "# from botorch.posteriors.torch import TorchPosterior\n",
    "# from botorch.posteriors.gpytorch import GPyTorchPosterior\n",
    "\n",
    "# from gpytorch.kernels import ScaleKernel\n",
    "# from gpytorch.kernels import RBFKernel\n",
    "\n",
    "# from gpytorch.likelihoods import Likelihood\n",
    "\n",
    "# ## bo\n",
    "# from torch.utils.data import DataLoader\n",
    "# from neural_process import NeuralProcess\n",
    "# from training import NeuralProcessTrainer\n",
    "# from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "# from utils import context_target_split\n",
    "# from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "# from botorch.sampling.stochastic_samplers import StochasticSampler\n",
    "# from botorch.acquisition.objective import IdentityMCObjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralProcess1(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Implements Neural Process for functions of arbitrary dimensions.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x_dim : int\n",
    "#         Dimension of x values.\n",
    "\n",
    "#     y_dim : int\n",
    "#         Dimension of y values.\n",
    "\n",
    "#     r_dim : int\n",
    "#         Dimension of output representation r.\n",
    "\n",
    "#     z_dim : int\n",
    "#         Dimension of latent variable z.\n",
    "\n",
    "#     h_dim : int\n",
    "#         Dimension of hidden layer in encoder and decoder.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, x_dim, y_dim, r_dim, z_dim, h_dim):\n",
    "#         super(NeuralProcess1, self).__init__()\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.r_dim = r_dim\n",
    "#         self.z_dim = z_dim\n",
    "#         self.h_dim = h_dim\n",
    "\n",
    "#         # Initialize networks\n",
    "#         self.xy_to_r = Encoder(x_dim, y_dim, h_dim, r_dim)\n",
    "#         self.r_to_mu_sigma = MuSigmaEncoder(r_dim, z_dim)\n",
    "#         self.xz_to_y = Decoder(x_dim, z_dim, h_dim, y_dim)\n",
    "\n",
    "#         self._num_outputs = 1\n",
    "\n",
    "#         ##\n",
    "#         self.mu_context = None\n",
    "#         self.sigma_context = None\n",
    "#         self.q_context = None\n",
    "#         self.z_sample = None\n",
    "\n",
    "#         self.likelihood = Likelihood\n",
    "\n",
    "#     def aggregate(self, r_i):\n",
    "#         \"\"\"\n",
    "#         Aggregates representations for every (x_i, y_i) pair into a single\n",
    "#         representation.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         r_i : torch.Tensor\n",
    "#             Shape (batch_size, num_points, r_dim)\n",
    "#         \"\"\"\n",
    "#         return torch.mean(r_i, dim=1)\n",
    "\n",
    "#     def xy_to_mu_sigma(self, x, y):\n",
    "#         \"\"\"\n",
    "#         Maps (x, y) pairs into the mu and sigma parameters defining the normal\n",
    "#         distribution of the latent variables z.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         x : torch.Tensor\n",
    "#             Shape (batch_size, num_points, x_dim)\n",
    "\n",
    "#         y : torch.Tensor\n",
    "#             Shape (batch_size, num_points, y_dim)\n",
    "#         \"\"\"\n",
    "#         batch_size, num_points, _ = x.size()\n",
    "#         # Flatten tensors, as encoder expects one dimensional inputs\n",
    "#         x_flat = x.view(batch_size * num_points, self.x_dim)\n",
    "#         y_flat = y.contiguous().view(batch_size * num_points, self.y_dim)\n",
    "#         # Encode each point into a representation r_i\n",
    "#         r_i_flat = self.xy_to_r(x_flat, y_flat)\n",
    "#         # Reshape tensors into batches\n",
    "#         r_i = r_i_flat.view(batch_size, num_points, self.r_dim)\n",
    "#         # Aggregate representations r_i into a single representation r\n",
    "#         r = self.aggregate(r_i)\n",
    "#         # Return parameters of distribution\n",
    "#         return self.r_to_mu_sigma(r)\n",
    "\n",
    "#     def forward(self, x_context, y_context, x_target, y_target=None):\n",
    "#         \"\"\"\n",
    "#         Given context pairs (x_context, y_context) and target points x_target,\n",
    "#         returns a distribution over target points y_target.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         x_context : torch.Tensor\n",
    "#             Shape (batch_size, num_context, x_dim). Note that x_context is a\n",
    "#             subset of x_target.\n",
    "\n",
    "#         y_context : torch.Tensor\n",
    "#             Shape (batch_size, num_context, y_dim)\n",
    "\n",
    "#         x_target : torch.Tensor\n",
    "#             Shape (batch_size, num_target, x_dim)\n",
    "\n",
    "#         y_target : torch.Tensor or None\n",
    "#             Shape (batch_size, num_target, y_dim). Only used during training.\n",
    "\n",
    "#         Note\n",
    "#         ----\n",
    "#         We follow the convention given in \"Empirical Evaluation of Neural\n",
    "#         Process Objectives\" where context is a subset of target points. This was\n",
    "#         shown to work best empirically.\n",
    "#         \"\"\"\n",
    "#         # Infer quantities from tensor dimensions\n",
    "#         batch_size, num_context, x_dim = x_context.size()\n",
    "#         _, num_target, _ = x_target.size()\n",
    "#         _, _, y_dim = y_context.size()\n",
    "\n",
    "#         if self.training:\n",
    "#             # Encode target and context (context needs to be encoded to\n",
    "#             # calculate kl term)\n",
    "#             mu_target, sigma_target = self.xy_to_mu_sigma(x_target, y_target)\n",
    "#             mu_context, sigma_context = self.xy_to_mu_sigma(x_context, y_context)\n",
    "#             # Sample from encoded distribution using reparameterization trick\n",
    "#             ## change to mvn\n",
    "#             # mu_target = mu_target.squeeze(0)\n",
    "#             # sigma_target = sigma_target.squeeze(0)\n",
    "#             # mu_context = mu_context.squeeze(0)\n",
    "#             # sigma_context = sigma_context.squeeze(0)\n",
    "#             q_target = Normal(mu_target, sigma_target)\n",
    "#             q_context = Normal(mu_context, sigma_context)\n",
    "#             z_sample = q_target.rsample()\n",
    "#             # Get parameters of output distribution\n",
    "#             y_pred_mu, y_pred_sigma = self.xz_to_y(x_target, z_sample)\n",
    "#             # y_pred_mu = y_pred_mu.squeeze(0)\n",
    "#             # y_pred_sigma = y_pred_sigma.squeeze(0)\n",
    "#             p_y_pred = MultivariateNormal(y_pred_mu, y_pred_sigma)\n",
    "\n",
    "#             return p_y_pred, q_target, q_context\n",
    "#         else:\n",
    "#             # At testing time, encode only context\n",
    "#             mu_context, sigma_context = self.xy_to_mu_sigma(x_context, y_context)\n",
    "#             # Sample from distribution based on context\n",
    "#             ## change to mvn\n",
    "#             q_context = Normal(mu_context, sigma_context)\n",
    "#             z_sample = q_context.rsample()\n",
    "#             # Predict target points based on context\n",
    "#             y_pred_mu, y_pred_sigma = self.xz_to_y(x_target, z_sample)\n",
    "#             ## change to mvn\n",
    "#             p_y_pred = MultivariateNormal(y_pred_mu, y_pred_sigma)\n",
    "\n",
    "#             return p_y_pred\n",
    "        \n",
    "#     def num_outputs(self):\n",
    "#         r\"\"\"The number of outputs of the model.\"\"\"\n",
    "#         return self._num_outputs\n",
    "    \n",
    "#     def set_context_for_posterior(self, x_context, y_context):\n",
    "#         # At testing time, encode only context\n",
    "#         self.mu_context, self.sigma_context = self.xy_to_mu_sigma(x_context, y_context)\n",
    "#         # Sample from distribution based on context\n",
    "#         ## change to mvn\n",
    "#         # self.mu_context = self.mu_context.squeeze(0)\n",
    "#         # self.sigma_context = self.sigma_context.squeeze(0)\n",
    "#         self.q_context = Normal(self.mu_context, self.sigma_context)\n",
    "#         self.z_sample = self.q_context.rsample()\n",
    "\n",
    "#     def posterior(self, X, posterior_transform=None):\n",
    "#         # # At testing time, encode only context\n",
    "#         # mu_context, sigma_context = self.xy_to_mu_sigma(x_context, y_context)\n",
    "#         # # Sample from distribution based on context\n",
    "#         # q_context = Normal(mu_context, sigma_context)\n",
    "#         # z_sample = q_context.rsample()\n",
    "#         # Predict target points based on context\n",
    "#         y_pred_mu, y_pred_sigma = self.xz_to_y(X, self.z_sample)\n",
    "#         # y_pred_mu = y_pred_mu.squeeze(0).squeeze(-1)\n",
    "#         # y_pred_sigma = y_pred_sigma.squeeze(0).squeeze(-1)\n",
    "#         ## change to mvn\n",
    "#         p_y_pred = MultivariateNormal(y_pred_mu, y_pred_sigma)\n",
    "#         posterior = TorchPosterior(p_y_pred)\n",
    "#         # p_y_pred = MultivariateNormal(y_pred_mu, y_pred_sigma)\n",
    "#         # posterior = GPyTorchPosterior(p_y_pred)\n",
    "#         return p_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NP model\n",
    "# x_dim = 20\n",
    "# y_dim = 1\n",
    "# r_dim = 50  # Dimension of representation of context points\n",
    "# z_dim = 50  # Dimension of sampled latent variable\n",
    "# h_dim = 50  # Dimension of hidden layers in encoder and decoder\n",
    "# f = NeuralProcess1(x_dim, y_dim, r_dim, z_dim, h_dim)\n",
    "# optimizer = torch.optim.Adam(f.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "# ##\n",
    "\n",
    "# train_x = get_initial_points(dim, n_init)\n",
    "# train_y = torch.tensor(\n",
    "#     [eval_objective(x) for x in train_x], dtype=dtype, device=device\n",
    "# ).unsqueeze(-1)\n",
    "# ackley_dataset = My_Ackley(train_x = train_x, train_y = train_y)\n",
    "# train_batch_size = 1\n",
    "# data_loader = DataLoader(ackley_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "# num_context = train_x.shape[0] // 4\n",
    "# num_target = train_x.shape[0] // 4\n",
    "# np_trainer = NeuralProcessTrainer(device, f, optimizer,\n",
    "#                                 num_context_range=(num_context, num_context),\n",
    "#                                 num_extra_target_range=(num_target, num_target), \n",
    "#                                 print_freq=100)\n",
    "\n",
    "# f.training = True\n",
    "# np_trainer.train(data_loader, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a batch\n",
    "# f.training = False\n",
    "# for batch in data_loader:\n",
    "#     break\n",
    "# x, y = batch\n",
    "# x_context, y_context, _, _ = context_target_split(x[0:1], y[0:1], \n",
    "#                                                 num_context, \n",
    "#                                                 num_target)\n",
    "# f.set_context_for_posterior(x_context, y_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x = torch.rand([1,40,20])\n",
    "# # model.posterior(test_x)\n",
    "# print(f.posterior(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = StochasticSampler(torch.Size([batch_size*2]), seed=1234)\n",
    "# obj = IdentityMCObjective()\n",
    "\n",
    "# ei = qExpectedImprovement(f, train_y.max(), sampler, obj)\n",
    "# candidate, acq_value = optimize_acqf(\n",
    "#     ei,\n",
    "#     bounds=torch.stack(\n",
    "#         [\n",
    "#             torch.zeros(dim, dtype=dtype, device=device),\n",
    "#             torch.ones(dim, dtype=dtype, device=device),\n",
    "#         ]\n",
    "#     ),\n",
    "#     q=batch_size*2, # The number of candidates\n",
    "#     num_restarts=NUM_RESTARTS,\n",
    "#     raw_samples=RAW_SAMPLES, # The number of samples for initialization.\n",
    "# )\n",
    "\n",
    "# Y_next = torch.tensor(\n",
    "#     [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "# ).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(candidate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botorch_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
