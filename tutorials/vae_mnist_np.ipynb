{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VAE MNIST example: BO in a latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian Optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets  # transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from botorch.models.transforms import Standardize, Normalize\n",
        "\n",
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem setup\n",
        "\n",
        "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
        "\n",
        "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
        "\\longrightarrow \\text{score}.$$\n",
        "\n",
        "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4 * 4 * 50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pretrained_dir() -> str:\n",
        "    \"\"\"\n",
        "    Get the directory of pretrained models, which are in the BoTorch repo.\n",
        "\n",
        "    Returns the location specified by PRETRAINED_LOCATION if that env\n",
        "    var is set; otherwise checks if we are in a likely part of the BoTorch\n",
        "    repo (botorch/botorch or botorch/tutorials) and returns the right path.\n",
        "    \"\"\"\n",
        "    if \"PRETRAINED_LOCATION\" in os.environ.keys():\n",
        "        return os.environ[\"PRETRAINED_LOCATION\"]\n",
        "    cwd = os.getcwd()\n",
        "    folder = os.path.basename(cwd)\n",
        "    # automated tests run from botorch folder\n",
        "    if folder == \"botorch\":  \n",
        "        return os.path.join(cwd, \"tutorials/pretrained_models/\")\n",
        "    # typical case (running from tutorial folder)\n",
        "    elif folder == \"tutorials\":\n",
        "        return os.path.join(cwd, \"pretrained_models/\")\n",
        "    raise FileNotFoundError(\"Could not figure out location of pretrained models.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_weights_path = os.path.join(get_pretrained_dir(), \"mnist_cnn.pt\")\n",
        "cnn_model = Net().to(dtype=dtype, device=device)\n",
        "cnn_state_dict = torch.load(cnn_weights_path, map_location=device)\n",
        "cnn_model.load_state_dict(cnn_state_dict);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "vae_weights_path = os.path.join(get_pretrained_dir(), \"mnist_vae.pt\")\n",
        "vae_model = VAE().to(dtype=dtype, device=device)\n",
        "vae_state_dict = torch.load(vae_weights_path, map_location=device)\n",
        "vae_model.load_state_dict(vae_state_dict);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score(y):\n",
        "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
        "    centered at the digit '3'.\n",
        "    \"\"\"\n",
        "    return torch.exp(-2 * (y - 3) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_image(x):\n",
        "    \"\"\"The input x is an image and an expected score \n",
        "    based on the CNN classifier and the scoring \n",
        "    function is returned.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        probs = torch.exp(cnn_model(x))  # b x 10\n",
        "        scores = score(\n",
        "            torch.arange(10, device=device, dtype=dtype)\n",
        "        ).expand(probs.shape)\n",
        "    return (probs * scores).sum(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(train_x):\n",
        "    with torch.no_grad():\n",
        "        decoded = vae_model.decode(train_x)\n",
        "    return decoded.view(train_x.shape[0], 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model initialization and initial random batch\n",
        "\n",
        "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from botorch.models.transforms import Standardize, Normalize\n",
        "from botorch import fit_gpytorch_mll\n",
        "\n",
        "d = 20\n",
        "bounds = torch.tensor([[-6.0] * d, [6.0] * d], device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "def gen_initial_data(n=10):\n",
        "    # generate training data\n",
        "    train_x = unnormalize(\n",
        "        torch.rand(n, d, device=device, dtype=dtype), \n",
        "        bounds=bounds\n",
        "    )\n",
        "    train_obj = score_image(decode(train_x)).unsqueeze(-1)\n",
        "    best_observed_value = train_obj.max().item()\n",
        "    return train_x, train_obj, best_observed_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_fitted_model(model, train_x, train_obj, num_context, num_target, state_dict=None):\n",
        "#     # fit model\n",
        "#     np_trainer = NeuralProcessTrainer(device, neuralprocess, optimizer,\n",
        "#                                   num_context_range=(num_context, num_context),\n",
        "#                                   num_extra_target_range=(num_target, num_target), \n",
        "#                                   print_freq=200)\n",
        "#     sampler = StochasticSampler(torch.Size([BATCH_SIZE]), seed=1234)\n",
        "#     obj = IdentityMCObjective()\n",
        "\n",
        "#     # model = SingleTaskGP(\n",
        "#     #     train_X=normalize(train_x, bounds), \n",
        "#     #     train_Y=train_obj,\n",
        "#     #     outcome_transform=Standardize(m=1)\n",
        "#     # )\n",
        "#     # if state_dict is not None:\n",
        "#     #     model.load_state_dict(state_dict)\n",
        "#     # mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
        "#     # mll.to(train_x)\n",
        "#     # fit_gpytorch_mll(mll)\n",
        "#     return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a helper function that performs the essential BO step\n",
        "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "\n",
        "BATCH_SIZE = 3 if not SMOKE_TEST else 2\n",
        "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
        "RAW_SAMPLES = 256 if not SMOKE_TEST else 4\n",
        "\n",
        "\n",
        "def optimize_acqf_and_get_observation(acq_func):\n",
        "    \"\"\"Optimizes the acquisition function, and returns a\n",
        "    new candidate and a noisy observation\"\"\"\n",
        "\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=torch.stack(\n",
        "            [\n",
        "                torch.zeros(d, dtype=dtype, device=device),\n",
        "                torch.ones(d, dtype=dtype, device=device),\n",
        "            ]\n",
        "        ),\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,\n",
        "    )\n",
        "\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
        "    new_obj = score_image(decode(new_x)).unsqueeze(-1)\n",
        "    return new_x, new_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform Bayesian Optimization loop with qEI\n",
        "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2048` samples. We also initialize the model with 5 randomly drawn points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "N_BATCH = 25 if not SMOKE_TEST else 3\n",
        "best_observed = []\n",
        "\n",
        "# call helper function to initialize model\n",
        "train_x, train_obj, best_value = gen_initial_data(n=10)\n",
        "best_observed.append(best_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20])\n",
            "torch.Size([10, 1])\n",
            "[0.6942477822303772]\n"
          ]
        }
      ],
      "source": [
        "print(train_x.shape)\n",
        "print(train_obj.shape)\n",
        "print(best_observed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to run the BO loop (this make take a few minutes, depending on your machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "from neural_process import NeuralProcess\n",
        "from training import NeuralProcessTrainer\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "from utils import context_target_split\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.sampling.stochastic_samplers import StochasticSampler\n",
        "from botorch.acquisition.objective import IdentityMCObjective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class My_mnist(Dataset):\n",
        "    def __init__(self, train_x, train_y):\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "        # Generate data\n",
        "        self.data = []\n",
        "        self.data.append((self.train_x, self.train_y))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10, 20])\n",
            "torch.Size([1, 10, 1])\n"
          ]
        }
      ],
      "source": [
        "train_batch_size = 1\n",
        "mnist_dataset = My_mnist(train_x = train_x, train_y = train_obj)\n",
        "data_loader = DataLoader(mnist_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "for _, i in enumerate(data_loader):\n",
        "    print(i[0].shape)\n",
        "    print(i[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running BO \n",
            "iteration 200, loss -4.248\n",
            "iteration 400, loss -5.849\n",
            "iteration 600, loss -6.632\n",
            "iteration 800, loss -5.773\n",
            "iteration 1000, loss -5.147\n",
            "iteration 1200, loss -6.327\n",
            "iteration 1400, loss -6.326\n",
            "iteration 1600, loss -4.900\n",
            "iteration 1800, loss -5.970\n",
            "iteration 2000, loss -6.430\n",
            "iteration 2200, loss -7.622\n",
            "iteration 2400, loss -9.483\n",
            "iteration 2600, loss -6.636\n",
            "iteration 2800, loss -9.242\n",
            "iteration 3000, loss -8.699\n",
            "iteration 3200, loss -10.328\n",
            "iteration 3400, loss -7.741\n",
            "iteration 3600, loss -9.191\n",
            "iteration 3800, loss -9.445\n",
            "iteration 4000, loss -10.250\n",
            "iteration 4200, loss -4.885\n",
            "iteration 4400, loss -8.167\n",
            "iteration 4600, loss -8.682\n",
            "iteration 4800, loss -9.930\n",
            "iteration 5000, loss -9.953\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "## NP model\n",
        "x_dim = 20\n",
        "y_dim = 1\n",
        "r_dim = 50  # Dimension of representation of context points\n",
        "z_dim = 50  # Dimension of sampled latent variable\n",
        "h_dim = 50  # Dimension of hidden layers in encoder and decoder\n",
        "neuralprocess = NeuralProcess(x_dim, y_dim, r_dim, z_dim, h_dim)\n",
        "optimizer = torch.optim.Adam(neuralprocess.parameters(), lr=3e-4)\n",
        "\n",
        "train_batch_size = 1\n",
        "num_context = 4\n",
        "num_target = 4\n",
        "np_trainer = NeuralProcessTrainer(device, neuralprocess, optimizer,\n",
        "                                  num_context_range=(num_context, num_context),\n",
        "                                  num_extra_target_range=(num_target, num_target), \n",
        "                                  print_freq=200)\n",
        "\n",
        "sampler = StochasticSampler(torch.Size([BATCH_SIZE]), seed=1234)\n",
        "obj = IdentityMCObjective()\n",
        "##\n",
        "\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "N_BATCH = 25 if not SMOKE_TEST else 3\n",
        "best_observed = []\n",
        "\n",
        "# call helper function to initialize model\n",
        "train_x, train_obj, best_value = gen_initial_data(n=10)\n",
        "best_observed.append(best_value)\n",
        "\n",
        "print(f\"\\nRunning BO \", end=\"\\n\")\n",
        "\n",
        "state_dict = None\n",
        "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "for iteration in range(N_BATCH):\n",
        "\n",
        "    train_x_normalized = normalize(train_x, bounds)\n",
        "    best_f = train_obj.max() \n",
        "    train_Y = (train_obj - train_obj.mean()) / train_obj.std()\n",
        "\n",
        "    mnist_dataset = My_mnist(train_x = train_x_normalized, train_y = train_obj)\n",
        "    data_loader = DataLoader(mnist_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "    if state_dict is not None:\n",
        "        neuralprocess.load_state_dict(state_dict)\n",
        "    neuralprocess.training = True\n",
        "    np_trainer.train(data_loader, 200)\n",
        "\n",
        "    # Create a batch\n",
        "    neuralprocess.training = False\n",
        "    for batch in data_loader:\n",
        "        break\n",
        "    x, y = batch\n",
        "    x_context, y_context, _, _ = context_target_split(x[0:1], y[0:1], \n",
        "                                                    num_context, \n",
        "                                                    num_target)\n",
        "    neuralprocess.set_context_for_posterior(x_context, y_context)\n",
        "\n",
        "    # define the qNEI acquisition function\n",
        "    qEI = qExpectedImprovement(neuralprocess, best_f, sampler, obj)\n",
        "    # qEI = qExpectedImprovement(\n",
        "    #     model=neuralprocess, best_f=train_obj.max()\n",
        "    # )\n",
        "\n",
        "    # optimize and get new observation\n",
        "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
        "\n",
        "    # update training points\n",
        "    train_x = torch.cat((train_x, new_x))\n",
        "    train_obj = torch.cat((train_obj, new_obj))\n",
        "\n",
        "    # update progress\n",
        "    best_value = train_obj.max().item()\n",
        "    best_observed.append(best_value)\n",
        "\n",
        "    state_dict = neuralprocess.state_dict()\n",
        "\n",
        "    # print(\"-\"*80, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Z,R,H_dim = 50 일 때는 loss=-10.8 이정도가 최고 성능인듯?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([85, 20])\n",
            "tensor([ 3.0916, -2.6483, -1.1632,  2.8162, -5.6486,  3.5983, -1.2344,  3.0525,\n",
            "         0.8341, -0.7347,  1.6642,  0.2960,  2.1914, -2.3382, -0.4375, -0.5402,\n",
            "         0.8697, -0.0240,  5.2450,  1.8671])\n",
            "tensor([ 4.3291, -3.7688,  6.0000,  3.8109, -5.3884,  5.1917,  1.1984,  3.6050,\n",
            "        -0.1298, -3.5627, -3.7628, -2.6237,  6.0000,  6.0000,  1.2798,  6.0000,\n",
            "         5.3079,  4.9976, -1.2984, -2.5588])\n"
          ]
        }
      ],
      "source": [
        "print(train_x.shape)\n",
        "print(train_x[0])\n",
        "print(train_x[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x1400 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
        "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
        "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
        "\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    b = torch.argmax(score_image(decode(train_x[: inds[i], :])), dim=0)\n",
        "    img = decode(train_x[b].view(1, -1)).squeeze().cpu()\n",
        "    ax.imshow(img, alpha=0.8, cmap=\"gray\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
