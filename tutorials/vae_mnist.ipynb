{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VAE MNIST example: BO in a latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian Optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets  # transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from botorch.models.transforms import Standardize, Normalize\n",
        "\n",
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem setup\n",
        "\n",
        "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
        "\n",
        "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
        "\\longrightarrow \\text{score}.$$\n",
        "\n",
        "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4 * 4 * 50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pretrained_dir() -> str:\n",
        "    \"\"\"\n",
        "    Get the directory of pretrained models, which are in the BoTorch repo.\n",
        "\n",
        "    Returns the location specified by PRETRAINED_LOCATION if that env\n",
        "    var is set; otherwise checks if we are in a likely part of the BoTorch\n",
        "    repo (botorch/botorch or botorch/tutorials) and returns the right path.\n",
        "    \"\"\"\n",
        "    if \"PRETRAINED_LOCATION\" in os.environ.keys():\n",
        "        return os.environ[\"PRETRAINED_LOCATION\"]\n",
        "    cwd = os.getcwd()\n",
        "    folder = os.path.basename(cwd)\n",
        "    # automated tests run from botorch folder\n",
        "    if folder == \"botorch\":  \n",
        "        return os.path.join(cwd, \"tutorials/pretrained_models/\")\n",
        "    # typical case (running from tutorial folder)\n",
        "    elif folder == \"tutorials\":\n",
        "        return os.path.join(cwd, \"pretrained_models/\")\n",
        "    raise FileNotFoundError(\"Could not figure out location of pretrained models.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_weights_path = os.path.join(get_pretrained_dir(), \"mnist_cnn.pt\")\n",
        "cnn_model = Net().to(dtype=dtype, device=device)\n",
        "cnn_state_dict = torch.load(cnn_weights_path, map_location=device)\n",
        "cnn_model.load_state_dict(cnn_state_dict);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "vae_weights_path = os.path.join(get_pretrained_dir(), \"mnist_vae.pt\")\n",
        "vae_model = VAE().to(dtype=dtype, device=device)\n",
        "vae_state_dict = torch.load(vae_weights_path, map_location=device)\n",
        "vae_model.load_state_dict(vae_state_dict);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score(y):\n",
        "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
        "    centered at the digit '3'.\n",
        "    \"\"\"\n",
        "    return torch.exp(-2 * (y - 3) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_image(x):\n",
        "    \"\"\"The input x is an image and an expected score \n",
        "    based on the CNN classifier and the scoring \n",
        "    function is returned.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        probs = torch.exp(cnn_model(x))  # b x 10\n",
        "        scores = score(\n",
        "            torch.arange(10, device=device, dtype=dtype)\n",
        "        ).expand(probs.shape)\n",
        "    return (probs * scores).sum(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(train_x):\n",
        "    with torch.no_grad():\n",
        "        decoded = vae_model.decode(train_x)\n",
        "    return decoded.view(train_x.shape[0], 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model initialization and initial random batch\n",
        "\n",
        "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from botorch.models.transforms import Standardize, Normalize\n",
        "from botorch import fit_gpytorch_mll\n",
        "\n",
        "d = 20\n",
        "bounds = torch.tensor([[-6.0] * d, [6.0] * d], device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "def gen_initial_data(n=10):\n",
        "    # generate training data\n",
        "    train_x = unnormalize(\n",
        "        torch.rand(n, d, device=device, dtype=dtype), \n",
        "        bounds=bounds\n",
        "    )\n",
        "    train_obj = score_image(decode(train_x)).unsqueeze(-1)\n",
        "    best_observed_value = train_obj.max().item()\n",
        "    return train_x, train_obj, best_observed_value\n",
        "\n",
        "\n",
        "def get_fitted_model(train_x, train_obj, state_dict=None):\n",
        "    # initialize and fit model\n",
        "    model = SingleTaskGP(\n",
        "        train_X=normalize(train_x, bounds), \n",
        "        train_Y=train_obj,\n",
        "        outcome_transform=Standardize(m=1)\n",
        "    )\n",
        "    if state_dict is not None:\n",
        "        model.load_state_dict(state_dict)\n",
        "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
        "    mll.to(train_x)\n",
        "    fit_gpytorch_mll(mll)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x = unnormalize(\n",
        "        torch.rand(5, d, device=device, dtype=dtype), \n",
        "        bounds=bounds\n",
        "    )\n",
        "print(train_x)\n",
        "\n",
        "print(decode(train_x).size())\n",
        "print(score_image(decode(train_x)).size())\n",
        "print(score_image(decode(train_x)))\n",
        "print(score_image(decode(train_x)).unsqueeze(-1))\n",
        "\n",
        "train_obj = score_image(decode(train_x)).unsqueeze(-1)\n",
        "print(train_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-6., -6., -6., -6., -6., -6., -6., -6., -6., -6., -6., -6., -6., -6.,\n",
            "         -6., -6., -6., -6., -6., -6.],\n",
            "        [ 6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
            "          6.,  6.,  6.,  6.,  6.,  6.]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "print(bounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a helper function that performs the essential BO step\n",
        "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "\n",
        "BATCH_SIZE = 3 if not SMOKE_TEST else 2\n",
        "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
        "RAW_SAMPLES = 256 if not SMOKE_TEST else 4\n",
        "\n",
        "\n",
        "def optimize_acqf_and_get_observation(acq_func):\n",
        "    \"\"\"Optimizes the acquisition function, and returns a\n",
        "    new candidate and a noisy observation\"\"\"\n",
        "\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=torch.stack(\n",
        "            [\n",
        "                torch.zeros(d, dtype=dtype, device=device),\n",
        "                torch.ones(d, dtype=dtype, device=device),\n",
        "            ]\n",
        "        ),\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,\n",
        "    )\n",
        "\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
        "    new_obj = score_image(decode(new_x)).unsqueeze(-1)\n",
        "    return new_x, new_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform Bayesian Optimization loop with qEI\n",
        "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2048` samples. We also initialize the model with 5 randomly drawn points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "N_BATCH = 25 if not SMOKE_TEST else 3\n",
        "best_observed = []\n",
        "\n",
        "# call helper function to initialize model\n",
        "train_x, train_obj, best_value = gen_initial_data(n=10)\n",
        "best_observed.append(best_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.9573824721730966]\n"
          ]
        }
      ],
      "source": [
        "print(best_observed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to run the BO loop (this make take a few minutes, depending on your machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running BO ........................."
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "print(f\"\\nRunning BO \", end=\"\")\n",
        "\n",
        "state_dict = None\n",
        "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "for iteration in range(N_BATCH):\n",
        "\n",
        "    # fit the model\n",
        "    model = get_fitted_model(\n",
        "        train_x=train_x,\n",
        "        train_obj=train_obj,\n",
        "        state_dict=state_dict,\n",
        "    )\n",
        "\n",
        "    # define the qNEI acquisition function\n",
        "    qEI = qExpectedImprovement(\n",
        "        model=model, best_f=train_obj.max()\n",
        "    )\n",
        "\n",
        "    # optimize and get new observation\n",
        "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
        "\n",
        "    # update training points\n",
        "    train_x = torch.cat((train_x, new_x))\n",
        "    train_obj = torch.cat((train_obj, new_obj))\n",
        "\n",
        "    # update progress\n",
        "    best_value = train_obj.max().item()\n",
        "    best_observed.append(best_value)\n",
        "\n",
        "    state_dict = model.state_dict()\n",
        "\n",
        "    print(\".\", end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x1400 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
        "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
        "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
        "\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    b = torch.argmax(score_image(decode(train_x[: inds[i], :])), dim=0)\n",
        "    img = decode(train_x[b].view(1, -1)).squeeze().cpu()\n",
        "    ax.imshow(img, alpha=0.8, cmap=\"gray\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
