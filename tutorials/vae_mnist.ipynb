{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VAE MNIST example: BO in a latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian Optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets  # transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from botorch.models.transforms import Standardize, Normalize\n",
        "\n",
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem setup\n",
        "\n",
        "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
        "\n",
        "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
        "\\longrightarrow \\text{score}.$$\n",
        "\n",
        "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4 * 4 * 50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pretrained_dir() -> str:\n",
        "    \"\"\"\n",
        "    Get the directory of pretrained models, which are in the BoTorch repo.\n",
        "\n",
        "    Returns the location specified by PRETRAINED_LOCATION if that env\n",
        "    var is set; otherwise checks if we are in a likely part of the BoTorch\n",
        "    repo (botorch/botorch or botorch/tutorials) and returns the right path.\n",
        "    \"\"\"\n",
        "    if \"PRETRAINED_LOCATION\" in os.environ.keys():\n",
        "        return os.environ[\"PRETRAINED_LOCATION\"]\n",
        "    cwd = os.getcwd()\n",
        "    folder = os.path.basename(cwd)\n",
        "    # automated tests run from botorch folder\n",
        "    if folder == \"botorch\":  \n",
        "        return os.path.join(cwd, \"tutorials/pretrained_models/\")\n",
        "    # typical case (running from tutorial folder)\n",
        "    elif folder == \"tutorials\":\n",
        "        return os.path.join(cwd, \"pretrained_models/\")\n",
        "    raise FileNotFoundError(\"Could not figure out location of pretrained models.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_weights_path = os.path.join(get_pretrained_dir(), \"mnist_cnn.pt\")\n",
        "cnn_model = Net().to(dtype=dtype, device=device)\n",
        "cnn_state_dict = torch.load(cnn_weights_path, map_location=device)\n",
        "cnn_model.load_state_dict(cnn_state_dict);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "vae_weights_path = os.path.join(get_pretrained_dir(), \"mnist_vae.pt\")\n",
        "vae_model = VAE().to(dtype=dtype, device=device)\n",
        "vae_state_dict = torch.load(vae_weights_path, map_location=device)\n",
        "vae_model.load_state_dict(vae_state_dict);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score(y):\n",
        "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
        "    centered at the digit '3'.\n",
        "    \"\"\"\n",
        "    return torch.exp(-2 * (y - 3) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_image(x):\n",
        "    \"\"\"The input x is an image and an expected score \n",
        "    based on the CNN classifier and the scoring \n",
        "    function is returned.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        probs = torch.exp(cnn_model(x))  # b x 10\n",
        "        scores = score(\n",
        "            torch.arange(10, device=device, dtype=dtype)\n",
        "        ).expand(probs.shape)\n",
        "    return (probs * scores).sum(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(train_x):\n",
        "    with torch.no_grad():\n",
        "        decoded = vae_model.decode(train_x)\n",
        "    return decoded.view(train_x.shape[0], 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model initialization and initial random batch\n",
        "\n",
        "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from botorch.models.transforms import Standardize, Normalize\n",
        "\n",
        "d = 20\n",
        "bounds = torch.tensor([[-6.0] * d, [6.0] * d], device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "def gen_initial_data(n=5):\n",
        "    # generate training data\n",
        "    train_x = unnormalize(\n",
        "        torch.rand(n, d, device=device, dtype=dtype), \n",
        "        bounds=bounds\n",
        "    )\n",
        "    train_obj = score_image(decode(train_x)).unsqueeze(-1)\n",
        "    best_observed_value = train_obj.max().item()\n",
        "    return train_x, train_obj, best_observed_value\n",
        "\n",
        "\n",
        "def get_fitted_model(train_x, train_obj, state_dict=None):\n",
        "    # initialize and fit model\n",
        "    model = SingleTaskGP(\n",
        "        train_X=normalize(train_x, bounds), \n",
        "        train_Y=train_obj,\n",
        "        outcome_transform=Standardize(m=1)\n",
        "    )\n",
        "    if state_dict is not None:\n",
        "        model.load_state_dict(state_dict)\n",
        "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
        "    mll.to(train_x)\n",
        "    fit_gpytorch_mll(mll)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a helper function that performs the essential BO step\n",
        "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "\n",
        "BATCH_SIZE = 3 if not SMOKE_TEST else 2\n",
        "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
        "RAW_SAMPLES = 256 if not SMOKE_TEST else 4\n",
        "\n",
        "\n",
        "def optimize_acqf_and_get_observation(acq_func):\n",
        "    \"\"\"Optimizes the acquisition function, and returns a\n",
        "    new candidate and a noisy observation\"\"\"\n",
        "\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=torch.stack(\n",
        "            [\n",
        "                torch.zeros(d, dtype=dtype, device=device),\n",
        "                torch.ones(d, dtype=dtype, device=device),\n",
        "            ]\n",
        "        ),\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,\n",
        "    )\n",
        "\n",
        "    # observe new values\n",
        "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
        "    new_obj = score_image(decode(new_x)).unsqueeze(-1)\n",
        "    return new_x, new_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform Bayesian Optimization loop with qEI\n",
        "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2048` samples. We also initialize the model with 5 randomly drawn points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "N_BATCH = 25 if not SMOKE_TEST else 3\n",
        "best_observed = []\n",
        "\n",
        "# call helper function to initialize model\n",
        "train_x, train_obj, best_value = gen_initial_data(n=5)\n",
        "best_observed.append(best_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.15793709485409566]\n"
          ]
        }
      ],
      "source": [
        "print(best_observed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to run the BO loop (this make take a few minutes, depending on your machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running BO ........................."
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "print(f\"\\nRunning BO \", end=\"\")\n",
        "\n",
        "state_dict = None\n",
        "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "for iteration in range(N_BATCH):\n",
        "\n",
        "    # fit the model\n",
        "    model = get_fitted_model(\n",
        "        train_x=train_x,\n",
        "        train_obj=train_obj,\n",
        "        state_dict=state_dict,\n",
        "    )\n",
        "\n",
        "    # define the qNEI acquisition function\n",
        "    qEI = qExpectedImprovement(\n",
        "        model=model, best_f=train_obj.max()\n",
        "    )\n",
        "\n",
        "    # optimize and get new observation\n",
        "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
        "\n",
        "    # update training points\n",
        "    train_x = torch.cat((train_x, new_x))\n",
        "    train_obj = torch.cat((train_obj, new_obj))\n",
        "\n",
        "    # update progress\n",
        "    best_value = train_obj.max().item()\n",
        "    best_observed.append(best_value)\n",
        "\n",
        "    state_dict = model.state_dict()\n",
        "\n",
        "    print(\".\", end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x1400 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
        "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
        "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
        "\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    b = torch.argmax(score_image(decode(train_x[: inds[i], :])), dim=0)\n",
        "    img = decode(train_x[b].view(1, -1)).squeeze().cpu()\n",
        "    ax.imshow(img, alpha=0.8, cmap=\"gray\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
