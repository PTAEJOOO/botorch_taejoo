{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82109\\anaconda3\\envs\\botorch_ex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "import collections\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from ANP import *\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition import qExpectedImprovement\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.generation import MaxPosteriorSampling\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.test_functions import Ackley\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.priors import HorseshoePrior\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "fun = Ackley(dim=20, negate=True).to(dtype=dtype, device=device)\n",
    "fun.bounds[0, :].fill_(-5)\n",
    "fun.bounds[1, :].fill_(10)\n",
    "dim = fun.dim\n",
    "lb, ub = fun.bounds\n",
    "print(dim)\n",
    "\n",
    "batch_size = 4\n",
    "n_init = 2 * dim\n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky\n",
    "\n",
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
    "N_CANDIDATES = min(5000, max(2000, 200 * dim)) if not SMOKE_TEST else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_objective(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return fun(unnormalize(x, fun.bounds))\n",
    "\n",
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP-EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_RESTARTS = 2\n",
    "# RAW_SAMPLES = 4\n",
    "# N_CANDIDATES = 4\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X_ei = get_initial_points(dim, n_init)\n",
    "Y_ei = torch.tensor(\n",
    "    [eval_objective(x) for x in X_ei], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "while len(Y_ei) < 400:\n",
    "    train_Y = (Y_ei - Y_ei.mean()) / Y_ei.std()\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    model = SingleTaskGP(X_ei, train_Y, likelihood=likelihood)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_mll(mll)\n",
    "\n",
    "    # Create a batch\n",
    "    ei = qExpectedImprovement(model, train_Y.max())\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        ei,\n",
    "        bounds=torch.stack(\n",
    "            [\n",
    "                torch.zeros(dim, dtype=dtype, device=device),\n",
    "                torch.ones(dim, dtype=dtype, device=device),\n",
    "            ]\n",
    "        ),\n",
    "        q=batch_size,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "    )\n",
    "\n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    # Append data\n",
    "    X_ei = torch.cat((X_ei, candidate), axis=0)\n",
    "    Y_ei = torch.cat((Y_ei, Y_next), axis=0)\n",
    "    # print(X_ei.size(), Y_ei.size())\n",
    "\n",
    "    # Print current status\n",
    "    print(f\"{len(X_ei)}) Best value: {Y_ei.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NP-MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from math import pi\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class My_Ackley(Dataset):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "\n",
    "        # Generate data\n",
    "        self.data = []\n",
    "        self.data.append((self.train_x, self.train_y))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 20])\n",
      "torch.Size([1, 40, 1])\n"
     ]
    }
   ],
   "source": [
    "X_np = get_initial_points(dim, n_init)\n",
    "Y_np = torch.tensor(\n",
    "    [eval_objective(x) for x in X_np], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "ackley_dataset = My_Ackley(train_x = X_np, train_y = Y_np)\n",
    "data_loader = DataLoader(ackley_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for _, i in enumerate(data_loader):\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## bo\n",
    "# from torch.utils.data import DataLoader\n",
    "# from neural_process import NeuralProcess\n",
    "# from training import NeuralProcessTrainer\n",
    "# from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "# from utils import context_target_split\n",
    "# from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "# from botorch.sampling.stochastic_samplers import StochasticSampler\n",
    "# from botorch.acquisition.objective import IdentityMCObjective\n",
    "\n",
    "# ## NP model\n",
    "# x_dim = 20\n",
    "# y_dim = 1\n",
    "# r_dim = 50  # Dimension of representation of context points\n",
    "# z_dim = 50  # Dimension of sampled latent variable\n",
    "# h_dim = 50  # Dimension of hidden layers in encoder and decoder\n",
    "# neuralprocess = NeuralProcess(x_dim, y_dim, r_dim, z_dim, h_dim)\n",
    "# optimizer = torch.optim.Adam(neuralprocess.parameters(), lr=3e-4)\n",
    "\n",
    "# train_batch_size = 1\n",
    "# num_context = 4\n",
    "# num_target = 4\n",
    "# np_trainer = NeuralProcessTrainer(device, neuralprocess, optimizer,\n",
    "#                                   num_context_range=(num_context, num_context),\n",
    "#                                   num_extra_target_range=(num_target, num_target), \n",
    "#                                   print_freq=200)\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# X_np = get_initial_points(dim, n_init)\n",
    "# Y_np = torch.tensor(\n",
    "#     [eval_objective(x) for x in X_np], dtype=dtype, device=device\n",
    "# ).unsqueeze(-1)\n",
    "\n",
    "# sampler = StochasticSampler(torch.Size([batch_size]), seed=1234)\n",
    "# obj = IdentityMCObjective()\n",
    "\n",
    "# while len(Y_np) < 400:\n",
    "#     best_f = Y_np.max() \n",
    "#     train_Y = (Y_np - Y_np.mean()) / Y_np.std()\n",
    "#     ackley_dataset = My_Ackley(train_x = X_np, train_y = train_Y)\n",
    "#     data_loader = DataLoader(ackley_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "#     neuralprocess.training = True\n",
    "#     np_trainer.train(data_loader, 200)\n",
    "\n",
    "#     # Create a batch\n",
    "#     neuralprocess.training = False\n",
    "#     for batch in data_loader:\n",
    "#         break\n",
    "#     x, y = batch\n",
    "#     x_context, y_context, _, _ = context_target_split(x[0:1], y[0:1], \n",
    "#                                                     num_context, \n",
    "#                                                     num_target)\n",
    "#     neuralprocess.set_context_for_posterior(x_context, y_context)\n",
    "\n",
    "#     ei = qExpectedImprovement(neuralprocess, best_f, sampler, obj)\n",
    "#     candidate, acq_value = optimize_acqf(\n",
    "#         ei,\n",
    "#         bounds=torch.stack(\n",
    "#             [\n",
    "#                 torch.zeros(dim, dtype=dtype, device=device),\n",
    "#                 torch.ones(dim, dtype=dtype, device=device),\n",
    "#             ]\n",
    "#         ),\n",
    "#         q=batch_size, # The number of candidates\n",
    "#         num_restarts=NUM_RESTARTS,\n",
    "#         raw_samples=RAW_SAMPLES, # The number of samples for initialization.\n",
    "#     )\n",
    "#     # break\n",
    "#     Y_next = torch.tensor(\n",
    "#         [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "#     ).unsqueeze(-1)\n",
    "\n",
    "#     # Append data\n",
    "#     X_np = torch.cat((X_np, candidate), axis=0)\n",
    "#     Y_np = torch.cat((Y_np, Y_next), axis=0)\n",
    "\n",
    "#     # Print current status\n",
    "#     print(f\"{len(X_np)}) Best value: {Y_np.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## bo\n",
    "# from torch.utils.data import DataLoader\n",
    "# from neural_process import NeuralProcess\n",
    "# from training import NeuralProcessTrainer\n",
    "# from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "# from utils import context_target_split\n",
    "# from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "# from botorch.sampling.stochastic_samplers import StochasticSampler\n",
    "# from botorch.acquisition.objective import IdentityMCObjective\n",
    "\n",
    "# ## NP model\n",
    "# x_dim = 20\n",
    "# y_dim = 1\n",
    "# r_dim = 50  # Dimension of representation of context points\n",
    "# z_dim = 50  # Dimension of sampled latent variable\n",
    "# h_dim = 50  # Dimension of hidden layers in encoder and decoder\n",
    "# neuralprocess = NeuralProcess(x_dim, y_dim, r_dim, z_dim, h_dim)\n",
    "# optimizer = torch.optim.Adam(neuralprocess.parameters(), lr=3e-4)\n",
    "\n",
    "# train_batch_size = 1\n",
    "# num_context = 4\n",
    "# num_target = 4\n",
    "# np_trainer = NeuralProcessTrainer(device, neuralprocess, optimizer,\n",
    "#                                   num_context_range=(num_context, num_context),\n",
    "#                                   num_extra_target_range=(num_target, num_target), \n",
    "#                                   print_freq=200)\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# X_np1 = get_initial_points(dim, n_init)\n",
    "# Y_np1 = torch.tensor(\n",
    "#     [eval_objective(x) for x in X_np1], dtype=dtype, device=device\n",
    "# ).unsqueeze(-1)\n",
    "\n",
    "# batch_size = 8\n",
    "\n",
    "# sampler = StochasticSampler(torch.Size([batch_size]), seed=1234)\n",
    "# obj = IdentityMCObjective()\n",
    "\n",
    "# while len(Y_np1) < 400:\n",
    "#     best_f = Y_np1.max() \n",
    "#     train_Y = (Y_np1 - Y_np1.mean()) / Y_np1.std()\n",
    "#     ackley_dataset = My_Ackley(train_x = X_np1, train_y = train_Y)\n",
    "#     data_loader = DataLoader(ackley_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "#     neuralprocess.training = True\n",
    "#     np_trainer.train(data_loader, 200)\n",
    "\n",
    "#     # Create a batch\n",
    "#     neuralprocess.training = False\n",
    "#     for batch in data_loader:\n",
    "#         break\n",
    "#     x, y = batch\n",
    "#     x_context, y_context, _, _ = context_target_split(x[0:1], y[0:1], \n",
    "#                                                     num_context, \n",
    "#                                                     num_target)\n",
    "#     neuralprocess.set_context_for_posterior(x_context, y_context)\n",
    "\n",
    "#     ei = qExpectedImprovement(neuralprocess, best_f, sampler, obj)\n",
    "#     candidate, acq_value = optimize_acqf(\n",
    "#         ei,\n",
    "#         bounds=torch.stack(\n",
    "#             [\n",
    "#                 torch.zeros(dim, dtype=dtype, device=device),\n",
    "#                 torch.ones(dim, dtype=dtype, device=device),\n",
    "#             ]\n",
    "#         ),\n",
    "#         q=batch_size, # The number of candidates\n",
    "#         num_restarts=NUM_RESTARTS,\n",
    "#         raw_samples=RAW_SAMPLES, # The number of samples for initialization.\n",
    "#     )\n",
    "\n",
    "#     Y_next = torch.tensor(\n",
    "#         [eval_objective(x) for x in candidate], dtype=dtype, device=device\n",
    "#     ).unsqueeze(-1)\n",
    "\n",
    "#     # Append data\n",
    "#     X_np1 = torch.cat((X_np1, candidate), axis=0)\n",
    "#     Y_np1 = torch.cat((Y_np1, Y_next), axis=0)\n",
    "\n",
    "#     # Print current status\n",
    "#     print(f\"{len(X_np1)}) Best value: {Y_np1.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "names = [\"GP-qEI\", \"NP-qEI-stochasticsampler-bc4\", \"NP-qEI-stochasticsampler-bc8\"] # , \"EI\", \"Sobol\"\n",
    "runs = [Y_ei, Y_np, Y_np1] # , Y_ei, Y_Sobol\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, run in zip(names, runs):\n",
    "    fx = np.maximum.accumulate(run.cpu())\n",
    "    plt.plot(fx, marker=\"\", lw=3)\n",
    "\n",
    "plt.plot([0, len(Y_ei)], [fun.optimal_value, fun.optimal_value], \"k--\", lw=3)\n",
    "plt.xlabel(\"Function value\", fontsize=18)\n",
    "plt.xlabel(\"Number of evaluations\", fontsize=18)\n",
    "plt.title(\"20D Ackley\", fontsize=24)\n",
    "plt.xlim([0, len(Y_ei)])\n",
    "plt.ylim([-15, 1])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend(\n",
    "    names + [\"Global optimal value\"],\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0, -0.08, 1, 1),\n",
    "    bbox_transform=plt.gcf().transFigure,\n",
    "    ncol=4,\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TR-NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class TurboState:\n",
    "#     dim: int\n",
    "#     batch_size: int\n",
    "#     length: float = 0.8\n",
    "#     length_min: float = 0.5**7\n",
    "#     length_max: float = 1.6\n",
    "#     failure_counter: int = 0\n",
    "#     failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "#     success_counter: int = 0\n",
    "#     success_tolerance: int = 10  # Note: The original paper uses 3\n",
    "#     best_value: float = -float(\"inf\")\n",
    "#     restart_triggered: bool = False\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         self.failure_tolerance = math.ceil(\n",
    "#             max([4.0 / self.batch_size, float(self.dim) / self.batch_size])\n",
    "#         )\n",
    "\n",
    "\n",
    "# def update_state(state, Y_next):\n",
    "#     if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "#         state.success_counter += 1\n",
    "#         state.failure_counter = 0\n",
    "#     else:\n",
    "#         state.success_counter = 0\n",
    "#         state.failure_counter += 1\n",
    "\n",
    "#     if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "#         state.length = min(2.0 * state.length, state.length_max)\n",
    "#         state.success_counter = 0\n",
    "#     elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "#         state.length /= 2.0\n",
    "#         state.failure_counter = 0\n",
    "\n",
    "#     state.best_value = max(state.best_value, max(Y_next).item())\n",
    "#     if state.length < state.length_min:\n",
    "#         state.restart_triggered = True\n",
    "#     return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_batch(\n",
    "#     state,\n",
    "#     model,  # GP model\n",
    "#     X,  # Evaluated points on the domain [0, 1]^d\n",
    "#     Y,  # Function values\n",
    "#     batch_size,\n",
    "#     n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "#     num_restarts=10,\n",
    "#     raw_samples=512,\n",
    "#     acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "# ):\n",
    "#     assert acqf in (\"ts\", \"ei\")\n",
    "#     assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "#     if n_candidates is None:\n",
    "#         n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "#     # Scale the TR to be proportional to the lengthscales\n",
    "#     x_center = X[Y.argmax(), :].clone()\n",
    "#     weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "#     weights = weights / weights.mean()\n",
    "#     weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "#     tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "#     tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "\n",
    "#     if acqf == \"ts\":\n",
    "#         dim = X.shape[-1]\n",
    "#         sobol = SobolEngine(dim, scramble=True)\n",
    "#         pert = sobol.draw(n_candidates).to(dtype=dtype, device=device)\n",
    "#         pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "#         # Create a perturbation mask\n",
    "#         prob_perturb = min(20.0 / dim, 1.0)\n",
    "#         mask = torch.rand(n_candidates, dim, dtype=dtype, device=device) <= prob_perturb\n",
    "#         ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "#         mask[ind, torch.randint(0, dim - 1, size=(len(ind),), device=device)] = 1\n",
    "\n",
    "#         # Create candidate points from the perturbations and the mask\n",
    "#         X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "#         X_cand[mask] = pert[mask]\n",
    "\n",
    "#         # Sample on the candidate points\n",
    "#         thompson_sampling = MaxPosteriorSampling(model=model, replacement=False)\n",
    "#         with torch.no_grad():  # We don't need gradients when using TS\n",
    "#             X_next = thompson_sampling(X_cand, num_samples=batch_size)\n",
    "\n",
    "#     elif acqf == \"ei\":\n",
    "#         ei = qExpectedImprovement(model, train_Y.max())\n",
    "#         X_next, acq_value = optimize_acqf(\n",
    "#             ei,\n",
    "#             bounds=torch.stack([tr_lb, tr_ub]),\n",
    "#             q=batch_size,\n",
    "#             num_restarts=num_restarts,\n",
    "#             raw_samples=raw_samples,\n",
    "#         )\n",
    "\n",
    "#     return X_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## bo\n",
    "# from torch.utils.data import DataLoader\n",
    "# from neural_process import NeuralProcess\n",
    "# from training import NeuralProcessTrainer\n",
    "# from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "# from utils import context_target_split\n",
    "# from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "# from botorch.sampling.stochastic_samplers import StochasticSampler\n",
    "# from botorch.acquisition.objective import IdentityMCObjective\n",
    "\n",
    "# ## NP model\n",
    "# x_dim = 20\n",
    "# y_dim = 1\n",
    "# r_dim = 50  # Dimension of representation of context points\n",
    "# z_dim = 50  # Dimension of sampled latent variable\n",
    "# h_dim = 50  # Dimension of hidden layers in encoder and decoder\n",
    "# neuralprocess = NeuralProcess(x_dim, y_dim, r_dim, z_dim, h_dim)\n",
    "# ##\n",
    "# neuralprocess.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "# ##\n",
    "# optimizer = torch.optim.Adam(neuralprocess.parameters(), lr=3e-4)\n",
    "\n",
    "# train_batch_size = 1\n",
    "# num_context = 4\n",
    "# num_target = 4\n",
    "# np_trainer = NeuralProcessTrainer(device, neuralprocess, optimizer,\n",
    "#                                   num_context_range=(num_context, num_context),\n",
    "#                                   num_extra_target_range=(num_target, num_target), \n",
    "#                                   print_freq=200)\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# X_turbo_np = get_initial_points(dim, n_init)\n",
    "# Y_turbo_np = torch.tensor(\n",
    "#     [eval_objective(x) for x in X_turbo_np], dtype=dtype, device=device\n",
    "# ).unsqueeze(-1)\n",
    "\n",
    "# state = TurboState(dim, batch_size=batch_size)\n",
    "\n",
    "# batch_size = 8\n",
    "\n",
    "# sampler = StochasticSampler(torch.Size([batch_size]), seed=1234)\n",
    "# obj = IdentityMCObjective()\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# while not state.restart_triggered:  # Run until TuRBO converges\n",
    "#     # Fit a GP model\n",
    "#     train_Y = (Y_turbo_np - Y_turbo_np.mean()) / Y_turbo_np.std()\n",
    "\n",
    "#     ackley_dataset = My_Ackley(train_x = X_turbo_np, train_y = train_Y)\n",
    "#     data_loader = DataLoader(ackley_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "#     neuralprocess.training = True\n",
    "#     np_trainer.train(data_loader, 200)\n",
    "\n",
    "#     # Create a batch\n",
    "#     neuralprocess.training = False\n",
    "#     for batch in data_loader:\n",
    "#         break\n",
    "#     x, y = batch\n",
    "#     x_context, y_context, _, _ = context_target_split(x[0:1], y[0:1], \n",
    "#                                                     num_context, \n",
    "#                                                     num_target)\n",
    "#     neuralprocess.set_context_for_posterior(x_context, y_context)\n",
    "    \n",
    "#     # Do the fitting and acquisition function optimization inside the Cholesky context\n",
    "#     with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "#         # Fit the model\n",
    "        \n",
    "\n",
    "#         # Create a batch\n",
    "#         X_next = generate_batch(\n",
    "#             state=state,\n",
    "#             model=neuralprocess,\n",
    "#             X=X_turbo_np,\n",
    "#             Y=train_Y,\n",
    "#             batch_size=batch_size,\n",
    "#             n_candidates=N_CANDIDATES,\n",
    "#             num_restarts=NUM_RESTARTS,\n",
    "#             raw_samples=RAW_SAMPLES,\n",
    "#             acqf=\"ei\",\n",
    "#         )\n",
    "\n",
    "#     Y_next = torch.tensor(\n",
    "#         [eval_objective(x) for x in X_next], dtype=dtype, device=device\n",
    "#     ).unsqueeze(-1)\n",
    "\n",
    "#     # Update state\n",
    "#     state = update_state(state=state, Y_next=Y_next)\n",
    "\n",
    "#     # Append data\n",
    "#     X_turbo_np = torch.cat((X_turbo_np, X_next), dim=0)\n",
    "#     Y_turbo_np = torch.cat((Y_turbo_np, Y_next), dim=0)\n",
    "\n",
    "#     # Print current status\n",
    "#     print(\n",
    "#         f\"{len(X_turbo_np)}) Best value: {state.best_value:.2e}, TR length: {state.length:.2e}\"\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botorch_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
